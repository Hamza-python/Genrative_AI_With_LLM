{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7021b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61c17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --disable-pip-version-check torch==2.1.2 torchdata==0.7.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7358f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers==4.27.2 datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7a2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529cfe3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3534a542d50b49869b2b581afc4acf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to C:/Users/Hamza Bhatti/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573ca5707fdd4910a5e3b60b675c778f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a9688141814cdcadb21b54241c5beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48646071a83499c9bf444865756e902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bcc3aacab640ceb3d3b1d4c8e85bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3b3ca12f4c4e3ea4e039cc78fd723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Hamza Bhatti/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ee57e7aeb04306a19ed8141e3c57ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hugingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(hugingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5623db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example:  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASE LINE HUMAN SUMMARY: \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example:  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASE LINE HUMAN SUMMARY: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example: ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASE LINE HUMAN SUMMARY: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1e40ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0097ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c9e6e10840444a89fd4720dedbf369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamza Bhatti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hamza Bhatti\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98d12514ccd41d38e09a58362c6f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d91f61925e4e97800fa2835e47e610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abc3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ace4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hamza Bhatti\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ENCODED SENTENCS: \n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE: \n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print('ENCODED SENTENCS: ')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE: ')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e609be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - WITHOUT PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BSELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENRATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f62956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ZERO SHOT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input constructed prompt insteaad of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BSELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENRATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b283df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ZERO SHOT:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BSELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ZERO SHOT:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "    \n",
    "    # Input constructed prompt insteaad of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BSELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENRATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0565ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "    \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "what was going on?\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6914401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Did you see the information on sales for last year? The sales review was made at our last board meeting. There's a great news for our shareholders. \n",
      "#Person2#: Yeah, I went over the figures this morning. We're finally started making money again! \n",
      "#Person1#: We've never sold this much before, our growth this year has been phenomenal. We have captured 8% of the market, up from last year by 120%. Our strategy is working! \n",
      "#Person2#: not only our annual sales brake a record, but our month by month earnings exceeded our forecast as well. After a two-year slump, we finally back in the black. \n",
      "#Person1#: The company's performance has improved since Wallace became president. He's made our little business into a real lucrative operation. Products are selling like hotcakes. \n",
      "#Person2#: I think our success is partly because of Wallace's contribution, and partly because of our new marketing strategy. Because of the new advertising campaign, we have become the top sellers in the field. \n",
      "#Person1#: Let's hope this stroke of good luck lasts. I hope we are over our financial difficulties for good. \n",
      "#Person2#: Let's hope it is! \n",
      "\n",
      "what was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 280\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb516a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# and #Person2# talk about the success of their business and attribute the success to Wallace's contribution and the new marketing strategy. They hope good luck can last.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ONE SHOT:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENRATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b04b8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "what was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffed6ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENRATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505cea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENRATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "genration_config = GenerationConfig(max_new_tokkens=50)\n",
    "# genration_config = GenerationConfig(max_new_tokkens=50, do_sample=True, temprature=0.1)\n",
    "# genration_config = GenerationConfig(max_new_tokkens=50, do_sample=True, temprature=0.5)\n",
    "# genration_config = GenerationConfig(max_new_tokkens=50, do_sample=True, temprature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENRATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed192c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
